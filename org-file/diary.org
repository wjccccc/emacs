* 2021
** <2021-08-21 å…­ 20:27> - k8s cluster

   :LOGBOOK:
   CLOCK: [2021-08-21 å…­ 20:28]--[2021-08-21 å…­ 20:30] =>  0:02
   :END:
*** k8s cluster install
   
**** å‰ææ­¥éª¤ï¼š
     æ°¸ä¹…å…³é—­ swap
     å†…æ ¸å…³é—­ ipv6
     å†…æ ¸å¯ç”¨ ipv4 ipv6 bridge
     å‡çº§å†…æ ¸åˆ°4.4ä»¥ä¸Šç‰ˆæœ¬ï¼Œ3.xæœ‰ä¸ç¨³å®šæƒ…å†µ

     kube-proxy å¼€å¯ipvsçš„å‰ç½®æ¡ä»¶
     modprobe br_netfilter

     cat > /etc/sysconfig/modules/ipvs.modules <<EOF
     #!/bin/bash
     modprobe --ip_vs
     modprobe --ip_vs_rr
     modprobe --ip_vs_wrr
     modprobe --ip_vs_sh
     modprobe --nf_conntrack_ipv4
     EOF

     chmod 755 ipvs.modules
     lsmod|grep -e ip_vs -e nf_conntrack_ipv4

**** å®‰è£…docker
    
***** ä¾èµ–
      yum install -y yum-utils device-mapper-persistent-data lvm2

      yum-config-manager \
       --add-repo \
       http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

       yum update -y && yum install -y docker-ce

      
***** åˆ›å»ºdockerç›®å½•
       mkdir /etc/docker
     
     
*****  é…ç½®daemon
       cat > /etc/docker/daemon.json <<EOF
       { 
	"exec-opts":["native.cgroupdriver-systemd"],
	"log-driver":"json-file",
	"log-opts":{
           "max-size":"100m"
	 }

       } # for elk 
     
       systemctl start docker
       systemctl enable docker

***** å®‰è£…kubeadminï¼ˆä¸»ä»é…ç½®ï¼‰
      cat << EOF > /etc/yum.repos.d/kubernets.repo
      [kubernetes]
      name=kubernets
      baseurl=http://mirrors.aliyun.com/kubernetes/yum/repo/kubernetes-el7-x86_64
      enable=1
      gpgcheck=0
      repo_gpgcheck=0
      gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
      http://mirrors.aliyun.com/kubernets/yum/doc/rpm-package-key.gpg
      EOF

      yum -y install kubadmin-1.15.1 kubectl-1.15.1 kubelet-1.15.1
      systemctl enable kubelet.service

      æ‰€æœ‰èŠ‚ç‚¹å®‰è£…è¿è¡Œã€‚ã€‚ã€‚

***** åˆå§‹åŒ–ä¸»èŠ‚ç‚¹
     kubadminä»google gceä¸Šæ‹‰å–é•œåƒï¼Œéœ€è¦ç§‘å­¦ä¸Šç½‘ï¼Œé€Ÿåº¦æ…¢
    
     æˆ–è€… å¯¼å…¥ä¸‹è½½å¥½çš„é•œåƒ

     #ï¼/bin/bash
    
     for i in $(cat list.images)
     do
	dokcer load -i $i
     done

     ---
     kubadmin config print init-defaults > kubeadm-config.yaml
    	 localAPIEndpoint:
		 advertiseAddress: xxxx(master ip)
	 kubernetesVersion: v1.xxx(ç‰ˆæœ¬ä¸€è‡´)
	 networking:
		 podSubnet:"10.244.0.0/16" ###å¿…é¡»æ­¤ç½‘æ®µï¼Œfor flannel network default
		 serviceSubnet:10.96.0.0/12
		
     ---

     apiVersion: kubeproxy.config.k8s.io/v1alpha1
     kind:kubeproxyConfiguration
     featureGate:
    	 SupportIPVSProxyMode:true

     mode:ipvs

     åˆå§‹åŒ–ï¼Œå¹¶è‡ªåŠ¨åŠæ³•è¯ä¹¦#for ha cluster 
     kubeadm init --config-kubeadm-config.yaml --experimental-upload-certs |tee kubadm-init.log


     mkdir -p $HOME/.kube
     sudo cp -i /etc/kubernets/admin.conf $HOME/.kube/config
     sudo chown $(id -u):$(id -g) $HOME/.kube/config


     kubectl get node


    
***** éƒ¨ç½²ç½‘ç»œï¼Œéœ€è¦ä¸€ä¸ªæ‰å¹³åŒ–ç½‘ç»œå­˜åœ¨
      kubectl apply -f http://raw.githubusercontent.com/coreos/flannel/master/Documention/kube-flannel.yaml

      kubectl create -f kube-flannel.yaml 

      kubectl get node -n kube-system  #ç³»ç»Ÿç»„å»ºæ‰€åœ¨çš„åç§°ç©ºé—´


      ifconfig æŸ¥çœ‹flannelç½‘å¡æ˜¯å¦å‡ºç°

      åŠ å…¥å…¶å®ƒçš„nodeï¼Œtail kubadm-init.logä¸­çš„åŠ å…¥èŠ‚ç‚¹çš„ä¿¡æ¯
      å…¶å®ƒnodeä¸Šæ‰§è¡Œ
      kubeadm join ... --token ... --discovery-token-ca-cert-hash ...

      masterä¸ŠæŸ¥çœ‹ç»“æœ
      kubectl get node

      kubectl get node -n kube-system -o wide # verbose info
      kubectl get node -n kube-system -w #watch status

      ...k8s clusteréƒ¨ç½²å®Œæ¯•

      ä¸‹é¢æµ‹è¯•



*** é›†ç¾¤èµ„æºåˆ†ç±»

**** åç§°ç©ºé—´çº§åˆ«
    
     ä¸¾ä¾‹ï¼škubeadm get pod -n kube-system
    
     æŸ¥çœ‹k8sç³»ç»Ÿç»„ä»¶ï¼Œéœ€è¦åˆ¶å®šåç§°ç©ºé—´ï¼Œå¦åˆ™é»˜è®¤ç©ºé—´æ˜¯default,æ— æ³•çœ‹åˆ°ä¿¡æ¯

***** å·¥ä½œè´Ÿè½½å‹èµ„æºï¼ˆworkloadï¼‰ï¼š
      Podï¼ŒReplicaSetï¼ŒDeploymentï¼Œ
      StatefulSetã€DaemonSetã€Jobã€CronJobï¼ˆReplicaionControlleråœ¨V.11ç‰ˆæœ¬è¢«åºŸå¼ƒï¼‰

***** æœåŠ¡å‘ç°å³è´Ÿè½½å‡è¡¡å‹èµ„æº(ServiceDiscovery LoadBalance)ï¼š
      Serviceã€Ingressã€ã€‚ã€‚ã€‚

***** é…ç½®ä¸å­˜å‚¨å‹èµ„æºï¼š
      Volumeï¼ˆå­˜å‚¨å·ï¼‰
      CSIï¼ˆå®¹å™¨å­˜å‚¨å€Ÿå£ï¼Œå¯ä»¥æ‰©å±•å„ç§å„æ ·çš„ç¬¬ä¸‰æ–¹å­˜å‚¨å·ï¼‰

***** ç‰¹æ®Šç±»å‹çš„å­˜å‚¨å·ï¼š
      ConfigMapï¼ˆé…ç½®ä¸­å¿ƒä½¿ç”¨ï¼Œé…ç½®æ–‡ä»¶çƒ­æ›´æ–°ï¼‰ã€Secretï¼ˆä¿æŒæ•æ„Ÿæ•°æ®ï¼‰
      DownwarAPIï¼ˆæŠŠå¤–éƒ¨ç¯å¢ƒä¸­çš„ä¿¡æ¯è¾“å‡ºç»™å®¹å™¨ï¼‰

**** é›†ç¾¤çº§åˆ«

     ä¸¾ä¾‹ï¼šroleï¼Œæ•´ä¸ªclusterå¯è§

***** NameSpace

***** Node

***** Role

***** ClusterRole

***** RoleBinding

***** ClusterRoleBinding



**** å…ƒæ•°æ®çº§åˆ«
     ä¸¾ä¾‹ï¼šHPAï¼Œç”¨äºCPUåˆ©ç”¨ç‡æ¥è¿›è¡Œé›†ç¾¤æ‰©å±•çš„æŒ‡æ ‡

***** HAP

***** PodTemplate

***** LmitRange
*** <2021-08-22 æ—¥ 21:54> - èµ„æºæ¸…å•
      :LOGBOOK:
      CLOCK: [2021-08-22 æ—¥ 21:55]--[2021-08-22 æ—¥ 22:06] =>  0:11
      :END:
      æŸ¥çœ‹å¸®åŠ©æ–‡æ¡£å­—æ®µè¯´æ˜
      kubectl explain pod.spec


      æ¸…å•ä¸¾ä¾‹

      ---pod.yaml---

      apiVersion:v1
      kind:Pod
      metedata:
       name:myappp-pod
       labels:
	app:myapp
	version:v1
      spec:
       container:
       - name :app1
	 image:hub.xxx/library/myapp1:v1
       - name :app2
	 image:hub.xxx/library/myapp2.v1


 -------è¿è¡Œ---

 kubectl apply -f pod.yaml


 çŠ¶æ€æ£€æŸ¥

 kubectl get pod
 kubectl describe pod myapp-pod
 kubectl log myapp-pod -c app2 # åˆ¶å®šå®¹å™¨æŸ¥çœ‹

 åˆ é™¤é—®é¢˜pod
 kubectl delete pod myapp-pods
*** <2021-08-25 Wed 16:48> - k8s æ¢é’ˆ
    :LOGBOOK:
    CLOCK: [2021-08-25 Wed 16:49]--[2021-08-25 Wed 16:51] =>  0:02
    :END:
    æŸ¥çœ‹k8s pod
    kubctl get pod

    è¿›podä¸­æŸ¥çœ‹å®¹å™¨
    kubectl exec xxxx-pod -c xxx container -it -- /bin/sh

    åˆ é™¤æ‰€æœ‰pod

    kubectl delete pod --all
   
    åˆ é™¤svc
    kubectl delete svc mydb mynginx ...

*** <2021-08-26 Thu 10:58> - èµ„æºç®¡ç†å™¨
    :LOGBOOK:
    CLOCK: [2021-08-26 Thu 14:26]
    CLOCK: [2021-08-26 Thu 10:59]--[2021-08-26 Thu 11:00] =>  0:01
    :END:
    RS æŸ¥çœ‹æ›´æ”¹ğŸ˜Šlabel

    kubectl get pod 
    kubectl get pod --show-labels
    kubectl label pod xxx-pod tier=xxx --overwrite=True


    Deployment yamlæ–‡ä»¶ä¸¾ä¾‹
   
    apiVersion: extensin/v1beta1
    kind:Deplyment
    metedata:
    name:nginx-deployment
    spec:
    replicas:3
    template:
    labels:
    app:nginx

    spec:
    container:
	    - name:nginx
	      image:nginx:1.7.9
	      ports:
	      - containerport:80

		åˆ›å»ºï¼š

		kubectl create -f nginx.yaml --record ###record è®°å½•æ“ä½œ

		æŸ¥çœ‹
		kubectl get deployment
		kubectl get rs
		kubectl log ...

		æ‰©å®¹

		kubectl scale deployment nginx-deployment --replicas 10


	    æ›´æ–°
	    kubectl set image deployment/nginx-deployment nginx=imagenew:v2

	    å›æ»š
	    kubectl rollout undo deployment/nginx-deployment 
	    kubectl rollout undo deployment/nginx-deployment --to-revision=2

	
	    æ›´æ–°çŠ¶æ€
	    kubectl rollout status deployment/nginx-deployment

	    kubectl rollout history ...


	    åˆ é™¤
	    kubectl delete daemonset --all
*** <2021-08-27 äº” 10:08> - k8s ingress
    :LOGBOOK:
    CLOCK: [2021-08-27 äº” 10:08]--[2021-08-27 äº” 10:09] =>  0:01
    :END:
    
    nginx-ingress
     
    ä¸‹è½½é•œåƒ docker pull xxx/kubernets-ingress-controll
    ä¿å­˜é•œåƒ docker save -o xxx.tar  xxx-images:v1
